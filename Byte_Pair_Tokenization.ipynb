{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AoTinmcUr_J4"
      },
      "outputs": [],
      "source": [
        "sampel_kannada_txt = \"\"\"ಡಿಕೋಡರ್ ಒಂದು ಎಂಬೆಡಿಂಗ್ ಲೇಯರ್ ಅನ್ನು ಒಳಗೊಂಡಿರುತ್ತದೆ, ನಂತರ ಬಹು ಡಿಕೋಡರ್ ಲೇಯರ್‌ಗಳು, ನಂತರ ಅನ್-ಎಂಬೆಡಿಂಗ್ ಲೇಯರ್.\n",
        "ಪ್ರತಿ ಡಿಕೋಡರ್ ಮೂರು ಪ್ರಮುಖ ಘಟಕಗಳನ್ನು ಒಳಗೊಂಡಿದೆ: ಒಂದು ಸಾಂದರ್ಭಿಕವಾಗಿ ಮುಖವಾಡದ ಸ್ವಯಂ-ಗಮನ ಯಾಂತ್ರಿಕತೆ, ಅಡ್ಡ-ಗಮನ ಕಾರ್ಯವಿಧಾನ ಮತ್ತು ಫೀಡ್-ಫಾರ್ವರ್ಡ್ ನರಮಂಡಲ. ಡಿಕೋಡರ್ ಎನ್‌ಕೋಡರ್‌ನಂತೆಯೇ ಕಾರ್ಯನಿರ್ವಹಿಸುತ್ತದೆ, ಆದರೆ ಹೆಚ್ಚುವರಿ ಗಮನ ಕಾರ್ಯವಿಧಾನವನ್ನು ಸೇರಿಸಲಾಗುತ್ತದೆ, ಅದು ಎನ್‌ಕೋಡರ್‌ಗಳಿಂದ ಉತ್ಪತ್ತಿಯಾಗುವ ಎನ್‌ಕೋಡಿಂಗ್‌ಗಳಿಂದ ಸಂಬಂಧಿತ ಮಾಹಿತಿಯನ್ನು ಸೆಳೆಯುತ್ತದೆ. ಈ ಕಾರ್ಯವಿಧಾನವನ್ನು ಎನ್‌ಕೋಡರ್-ಡಿಕೋಡರ್ ಗಮನ ಎಂದೂ ಕರೆಯಬಹುದು\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "class BytePairEncoding:\n",
        "  def __init__(self, txt: str):\n",
        "\n",
        "    #getting all characters in list in sorted order\n",
        "    self.chars = sorted(list(set(txt)))\n",
        "\n",
        "    #dictionary for strings to integers\n",
        "    self.stoi = {ch: i for i, ch in enumerate(self.chars)}\n",
        "\n",
        "    #dictionary for integers to strings\n",
        "    self.itos = {i: ch for i, ch in enumerate(self.chars)}\n",
        "\n",
        "    # inital form of text to integers in terms of list\n",
        "    self.data = [self.stoi[c] for c in txt]\n",
        "\n",
        "    # Statistics tracking\n",
        "    self.stats = {\n",
        "        \"vocab_sizes\": [len(self.chars)],\n",
        "        \"data_sizes\": [len(self.data)],\n",
        "        \"compression_ratios\": [1.0],\n",
        "        \"merge_counts\": [],\n",
        "        \"tokens_created\": [],\n",
        "        \"max_token_lengths\": [1],\n",
        "    }\n",
        "\n",
        "    #length of characters of input txt\n",
        "    self.max_token_length = 0\n",
        "    self.orginal_txt_length = len(self.data)\n",
        "\n",
        "  def get_stats(self):\n",
        "    \"counting pair of characters\"\n",
        "    counts = Counter()\n",
        "    for pair in zip(self.data, self.data[1:]):\n",
        "      pair = (int(pair[0]), int(pair[1]))\n",
        "      counts[pair] += 1\n",
        "    return counts\n",
        "\n",
        "  def merge_pair_encoding(self, pair, idx):\n",
        "    \"replace tokens into newly defined token\"\n",
        "    newids = []\n",
        "    i = 0\n",
        "    while i < len(self.data):\n",
        "      if i < len(self.data) - 1 and self.data[i] == pair[0] and self.data[i+1] == pair[1]:\n",
        "        newids.append(idx)\n",
        "        i += 2\n",
        "      else:\n",
        "        newids.append(self.data[i])\n",
        "        i += 1\n",
        "    return newids\n",
        "\n",
        "  def add_new_encode_pair(self, pair: tuple[int, int]) -> int:\n",
        "      \"\"\"Add a new token to vocabulary dictionary\"\"\"\n",
        "      pair_str = self.itos[pair[0]] + self.itos[pair[1]]\n",
        "      next_idx = len(self.itos)\n",
        "      self.stoi[pair_str] = next_idx\n",
        "      self.itos[next_idx] = pair_str\n",
        "\n",
        "      # Update max token length\n",
        "      self.max_token_length = max(self.max_token_length, len(pair_str))\n",
        "      return next_idx\n",
        "\n",
        "  def update_stats(self, merge_count: int, new_token: str):\n",
        "    \"\"\"Record statistics after each merge operation\"\"\"\n",
        "    self.stats[\"vocab_sizes\"].append(len(self.itos))\n",
        "    self.stats[\"data_sizes\"].append(len(self.data))\n",
        "    self.stats[\"compression_ratios\"].append(self.orginal_txt_length / len(self.data))\n",
        "    self.stats[\"merge_counts\"].append(merge_count)\n",
        "    self.stats[\"tokens_created\"].append(new_token)\n",
        "\n",
        "\n",
        "  def merge_byte_pair(self) -> tuple[int, str, int]:\n",
        "      \"\"\"\n",
        "      Merge the top byte pair\n",
        "\n",
        "      Returns:\n",
        "          tuple: (new_token_id, new_token_str, merge_count) or None if no more pairs to merge\n",
        "      \"\"\"\n",
        "      # Get pair frequencies\n",
        "      stats = self.get_stats()\n",
        "      if not stats:  # No more pairs to merge\n",
        "          return None\n",
        "\n",
        "      # Find most frequent pair\n",
        "      (top_pair, count) = max(stats.items(), key=lambda x: x[1])\n",
        "\n",
        "      # Add new token to vocabulary\n",
        "      new_idx = self.add_new_encode_pair(top_pair)\n",
        "\n",
        "      # Replace pairs in data\n",
        "      self.data = self.merge_pair_encoding(top_pair, new_idx)\n",
        "\n",
        "      # Update statistics\n",
        "      self.update_stats(count, self.itos[new_idx])\n",
        "\n",
        "      return new_idx, self.itos[new_idx], count\n",
        "\n",
        "  def print_progress(self, iteration: int, new_token: str, merge_count: int):\n",
        "    \"\"\"\n",
        "    Print training progress in text format\n",
        "\n",
        "    Args:\n",
        "        iteration: Current iteration number\n",
        "        new_token: Newly created token\n",
        "        merge_count: Number of merges for this token\n",
        "    \"\"\"\n",
        "    print(f\"\\nIteration {iteration:,}\")\n",
        "    print(f\"Current vocabulary size: {len(self.itos):,}\")\n",
        "    print(f\"Current data size: {len(self.data):,}\")\n",
        "    print(f\"Current compression ratio: {self.stats['compression_ratios'][-1]:.2f}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "  def encode_to_vocab_size(\n",
        "        self,\n",
        "        target_vocab_size: int,\n",
        "        print_interval: int = 100,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Perform BPE encoding until reaching target vocabulary size\n",
        "\n",
        "        Args:\n",
        "            target_vocab_size: Maximum vocabulary size to reach\n",
        "            print_interval: How often to print progress (None for no printing)\n",
        "        \"\"\"\n",
        "        pbar = tqdm(\n",
        "            total=target_vocab_size,\n",
        "            desc=f\"Encoding byte pairs\",\n",
        "            initial=len(self.chars),\n",
        "            position=0,\n",
        "            leave=True,\n",
        "        )\n",
        "\n",
        "        iteration = 0\n",
        "        while len(self.itos) < target_vocab_size:\n",
        "            # Train one iteration\n",
        "            result = self.merge_byte_pair()\n",
        "            if result is None:  # No more pairs to merge\n",
        "                break\n",
        "\n",
        "            new_idx, new_token, merge_count = result\n",
        "            iteration += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.update(1)\n",
        "\n",
        "            # Print progress at intervals if requested\n",
        "            if print_interval and iteration % print_interval == 0:\n",
        "                self.print_progress(iteration, new_token, merge_count)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "        # Final statistics\n",
        "        print(f\"\\nTraining completed after {iteration:,} iterations\")\n",
        "        print(f\"Final vocabulary size: {len(self.itos):,}\")\n",
        "\n",
        "  def encode(self, text: str) -> list[int]:\n",
        "    \"\"\"Convert text to token indices\"\"\"\n",
        "    return [self.stoi[c] for c in text]\n",
        "\n",
        "  def decode(self, token_ids: list[int]) -> str:\n",
        "    \"\"\"Convert token indices back to text\"\"\"\n",
        "    return \"\".join(self.itos[idx] for idx in token_ids)"
      ],
      "metadata": {
        "id": "Cd_a5eHt3I2q"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_enc = BytePairEncoding(sampel_kannada_txt)\n",
        "sample_enc.encode_to_vocab_size(target_vocab_size = 300, print_interval=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V90sLfqx7x3j",
        "outputId": "f16a7a34-8570-44fd-bc3b-7a3263bb82c3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding byte pairs: 100%|██████████| 300/300 [00:00<00:00, 2176.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iteration 10\n",
            "Current vocabulary size: 57\n",
            "Current data size: 392\n",
            "Current compression ratio: 1.24\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 20\n",
            "Current vocabulary size: 67\n",
            "Current data size: 344\n",
            "Current compression ratio: 1.42\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 30\n",
            "Current vocabulary size: 77\n",
            "Current data size: 307\n",
            "Current compression ratio: 1.59\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 40\n",
            "Current vocabulary size: 87\n",
            "Current data size: 277\n",
            "Current compression ratio: 1.76\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 50\n",
            "Current vocabulary size: 97\n",
            "Current data size: 250\n",
            "Current compression ratio: 1.95\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 60\n",
            "Current vocabulary size: 107\n",
            "Current data size: 230\n",
            "Current compression ratio: 2.12\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 70\n",
            "Current vocabulary size: 117\n",
            "Current data size: 210\n",
            "Current compression ratio: 2.32\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 80\n",
            "Current vocabulary size: 127\n",
            "Current data size: 191\n",
            "Current compression ratio: 2.55\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 90\n",
            "Current vocabulary size: 137\n",
            "Current data size: 181\n",
            "Current compression ratio: 2.69\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 100\n",
            "Current vocabulary size: 147\n",
            "Current data size: 171\n",
            "Current compression ratio: 2.85\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 110\n",
            "Current vocabulary size: 157\n",
            "Current data size: 161\n",
            "Current compression ratio: 3.02\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 120\n",
            "Current vocabulary size: 167\n",
            "Current data size: 151\n",
            "Current compression ratio: 3.23\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 130\n",
            "Current vocabulary size: 177\n",
            "Current data size: 141\n",
            "Current compression ratio: 3.45\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 140\n",
            "Current vocabulary size: 187\n",
            "Current data size: 131\n",
            "Current compression ratio: 3.72\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 150\n",
            "Current vocabulary size: 197\n",
            "Current data size: 121\n",
            "Current compression ratio: 4.02\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 160\n",
            "Current vocabulary size: 207\n",
            "Current data size: 111\n",
            "Current compression ratio: 4.39\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 170\n",
            "Current vocabulary size: 217\n",
            "Current data size: 101\n",
            "Current compression ratio: 4.82\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 180\n",
            "Current vocabulary size: 227\n",
            "Current data size: 91\n",
            "Current compression ratio: 5.35\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 190\n",
            "Current vocabulary size: 237\n",
            "Current data size: 81\n",
            "Current compression ratio: 6.01\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 200\n",
            "Current vocabulary size: 247\n",
            "Current data size: 71\n",
            "Current compression ratio: 6.86\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 210\n",
            "Current vocabulary size: 257\n",
            "Current data size: 61\n",
            "Current compression ratio: 7.98\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 220\n",
            "Current vocabulary size: 267\n",
            "Current data size: 51\n",
            "Current compression ratio: 9.55\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 230\n",
            "Current vocabulary size: 277\n",
            "Current data size: 41\n",
            "Current compression ratio: 11.88\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 240\n",
            "Current vocabulary size: 287\n",
            "Current data size: 31\n",
            "Current compression ratio: 15.71\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Iteration 250\n",
            "Current vocabulary size: 297\n",
            "Current data size: 21\n",
            "Current compression ratio: 23.19\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Training completed after 253 iterations\n",
            "Final vocabulary size: 300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(set(sampel_kannada_txt)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i5ZzNx39iST",
        "outputId": "ecedc2ee-5951-457f-8480-2930b6bec958"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(sampel_kannada_txt)))\n",
        "chars"
      ],
      "metadata": {
        "id": "Gnkcx1_4IXs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "stoi"
      ],
      "metadata": {
        "id": "M0pD5DtVIm27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [stoi[c] for c in sampel_kannada_txt]\n",
        "data"
      ],
      "metadata": {
        "id": "JGuqaQq7Iq2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ele in sampel_kannada_txt[:10]:\n",
        "  print(f\"Kannada letter is {ele} and its ordinal is {ord(ele)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJMeWzs1sp4C",
        "outputId": "c4964ffe-805a-4ec0-94ac-51d0eb3f4fff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kannada letter is ಡ and its ordinal is 3233\n",
            "Kannada letter is ಿ and its ordinal is 3263\n",
            "Kannada letter is ಕ and its ordinal is 3221\n",
            "Kannada letter is ೋ and its ordinal is 3275\n",
            "Kannada letter is ಡ and its ordinal is 3233\n",
            "Kannada letter is ರ and its ordinal is 3248\n",
            "Kannada letter is ್ and its ordinal is 3277\n",
            "Kannada letter is   and its ordinal is 32\n",
            "Kannada letter is ಒ and its ordinal is 3218\n",
            "Kannada letter is ಂ and its ordinal is 3202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [ele for ele in sampel_kannada_txt]"
      ],
      "metadata": {
        "id": "ajhSeJbUtpOd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "  newids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      newids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i += 1\n",
        "  return newids\n",
        "\n",
        "# ---\n",
        "vocab_size = 500 # the desired final vocabulary size\n",
        "num_merges = vocab_size - 256\n",
        "ids = list(tokens) # copy so we don't destroy the original list\n",
        "\n",
        "merges = {} # (int, int) -> int\n",
        "for i in range(num_merges):\n",
        "  stats = get_stats(ids)\n",
        "  pair = max(stats, key=stats.get)\n",
        "  idx = 256 + i\n",
        "  print(f\"merging {pair} into a new token {idx}\")\n",
        "  ids = merge(ids, pair, idx)\n",
        "  merges[pair] = idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOm6CuIhs3N-",
        "outputId": "796a9321-9c0a-4c2a-fef0-b81a1da8a0c8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging ('ರ', '್') into a new token 256\n",
            "merging ('ನ', '್') into a new token 257\n",
            "merging ('ು', ' ') into a new token 258\n",
            "merging ('ಡ', 'ಿ') into a new token 259\n",
            "merging ('ಕ', 'ೋ') into a new token 260\n",
            "merging ('ತ', '್') into a new token 261\n",
            "merging (' ', 'ಮ') into a new token 262\n",
            "merging (260, 'ಡ') into a new token 263\n",
            "merging (263, 256) into a new token 264\n",
            "merging (' ', 'ಅ') into a new token 265\n",
            "merging (257, 'ನ') into a new token 266\n",
            "merging ('ಾ', 'ಗ') into a new token 267\n",
            "merging (266, 258) into a new token 268\n",
            "merging ('ಟ', '್') into a new token 269\n",
            "merging (261, 'ತ') into a new token 270\n",
            "merging ('ದ', 'ೆ') into a new token 271\n",
            "merging ('ಂ', 'ದ') into a new token 272\n",
            "merging (',', ' ') into a new token 273\n",
            "merging ('ರ', 'ಿ') into a new token 274\n",
            "merging (257, '\\u200c') into a new token 275\n",
            "merging (259, 264) into a new token 276\n",
            "merging ('ು', 270) into a new token 277\n",
            "merging (277, 271) into a new token 278\n",
            "merging ('ಗ', 'ಳ') into a new token 279\n",
            "merging ('್', 'ರ') into a new token 280\n",
            "merging ('ಸ', '್') into a new token 281\n",
            "merging (259, 'ಂ') into a new token 282\n",
            "merging (282, 'ಗ') into a new token 283\n",
            "merging (283, '್') into a new token 284\n",
            "merging (267, 'ಿ') into a new token 285\n",
            "merging ('ಗ', 'ಮ') into a new token 286\n",
            "merging (286, 'ನ') into a new token 287\n",
            "merging ('ಾ', 256) into a new token 288\n",
            "merging ('ವ', 'ಿ') into a new token 289\n",
            "merging ('ಎ', 275) into a new token 290\n",
            "merging ('ಲ', '್') into a new token 291\n",
            "merging (276, ' ') into a new token 292\n",
            "merging ('\\u200c', 279) into a new token 293\n",
            "merging (' ', 'ಕ') into a new token 294\n",
            "merging ('ಯ', 289) into a new token 295\n",
            "merging ('ಾ', 'ನ') into a new token 296\n",
            "merging ('.', ' ') into a new token 297\n",
            "merging ('ಿ', 'ಸ') into a new token 298\n",
            "merging ('ರ', 'ೆ') into a new token 299\n",
            "merging ('ಪ', 'ು') into a new token 300\n",
            "merging (300, 269) into a new token 301\n",
            "merging (291, 'ಲ') into a new token 302\n",
            "merging ('ಪ', 280) into a new token 303\n",
            "merging ('ತ', 'ಿ') into a new token 304\n",
            "merging ('ಡ', '್') into a new token 305\n",
            "merging (294, 288) into a new token 306\n",
            "merging (290, 264) into a new token 307\n",
            "merging ('ಗ', 'ೆ') into a new token 308\n",
            "merging ('ಔ', 269) into a new token 309\n",
            "merging (309, '\\u200c') into a new token 310\n",
            "merging (310, 301) into a new token 311\n",
            "merging ('ಕ', '್') into a new token 312\n",
            "merging ('ಂ', 'ಬ') into a new token 313\n",
            "merging (265, 268) into a new token 314\n",
            "merging ('ನ', 'ಂ') into a new token 315\n",
            "merging (315, 'ತ') into a new token 316\n",
            "merging ('ವ', 285) into a new token 317\n",
            "merging ('ದ', ' ') into a new token 318\n",
            "merging ('-', 287) into a new token 319\n",
            "merging (306, 295) into a new token 320\n",
            "merging (320, 'ಧ') into a new token 321\n",
            "merging (321, 296) into a new token 322\n",
            "merging (' ', 'ಹ') into a new token 323\n",
            "merging ('ು', 'ವ') into a new token 324\n",
            "merging ('ವ', 268) into a new token 325\n",
            "merging (260, 284) into a new token 326\n",
            "merging (262, 'ಾ') into a new token 327\n",
            "merging (278, 297) into a new token 328\n",
            "merging ('ದ', 'ಲ') into a new token 329\n",
            "merging ('ಎ', 313) into a new token 330\n",
            "merging (330, 'ೆ') into a new token 331\n",
            "merging (331, 284) into a new token 332\n",
            "merging ('ಲ', 'ೇ') into a new token 333\n",
            "merging (333, 'ಯ') into a new token 334\n",
            "merging (334, 256) into a new token 335\n",
            "merging (278, 273) into a new token 336\n",
            "merging ('ರ', ' ') into a new token 337\n",
            "merging (258, 'ಸ') into a new token 338\n",
            "merging ('ಿ', 'ಕ') into a new token 339\n",
            "merging (281, 'ವ') into a new token 340\n",
            "merging (340, 'ಯ') into a new token 341\n",
            "merging (341, 'ಂ') into a new token 342\n",
            "merging (',', 265) into a new token 343\n",
            "merging (307, '\\u200c') into a new token 344\n",
            "merging ('ೆ', 'ಯ') into a new token 345\n",
            "merging (293, 'ಿ') into a new token 346\n",
            "merging (272, ' ') into a new token 347\n",
            "merging (327, 'ಹ') into a new token 348\n",
            "merging (348, 'ಿ') into a new token 349\n",
            "merging (349, 304) into a new token 350\n",
            "merging ('ದ', 'ು') into a new token 351\n",
            "merging ('ೊ', 329) into a new token 352\n",
            "merging ('ಕ', 280) into a new token 353\n",
            "merging ('್', 'ಯ') into a new token 354\n",
            "merging (262, 299) into a new token 355\n",
            "merging (355, 'ಮ') into a new token 356\n",
            "merging (356, 'ಾ') into a new token 357\n",
            "merging ('ಇ', 'ದ') into a new token 358\n",
            "merging ('ಒ', 272) into a new token 359\n",
            "merging (258, 332) into a new token 360\n",
            "merging (' ', 335) into a new token 361\n",
            "merging ('ಒ', 'ಳ') into a new token 362\n",
            "merging (362, 'ಗ') into a new token 363\n",
            "merging (363, 'ೊ') into a new token 364\n",
            "merging (364, 'ಂ') into a new token 365\n",
            "merging (365, 259) into a new token 366\n",
            "merging ('ಬ', 'ಹ') into a new token 367\n",
            "merging ('.', '\\n') into a new token 368\n",
            "merging ('ು', 'ಖ') into a new token 369\n",
            "merging (338, 'ಾ') into a new token 370\n",
            "merging (272, 256) into a new token 371\n",
            "merging (371, 'ಭ') into a new token 372\n",
            "merging (372, 339) into a new token 373\n",
            "merging (373, 317) into a new token 374\n",
            "merging ('ವ', 'ಾ') into a new token 375\n",
            "merging (342, 319) into a new token 376\n",
            "merging ('ತ', 'ೆ') into a new token 377\n",
            "merging (343, 305) into a new token 378\n",
            "merging (378, 'ಡ') into a new token 379\n",
            "merging (379, 319) into a new token 380\n",
            "merging (380, 322) into a new token 381\n",
            "merging (262, 270) into a new token 382\n",
            "merging ('ಫ', 288) into a new token 383\n",
            "merging ('ವ', 256) into a new token 384\n",
            "merging (' ', 'ನ') into a new token 385\n",
            "merging ('ಡ', 'ಲ') into a new token 386\n",
            "merging (344, 316) into a new token 387\n",
            "merging ('ಹ', 298) into a new token 388\n",
            "merging ('ಆ', 'ದ') into a new token 389\n",
            "merging ('ಚ', 324) into a new token 390\n",
            "merging (322, 325) into a new token 391\n",
            "merging ('ೇ', 274) into a new token 392\n",
            "merging ('ಲ', 267) into a new token 393\n",
            "merging (346, 347) into a new token 394\n",
            "merging ('ಉ', 261) into a new token 395\n",
            "merging (395, 'ಪ') into a new token 396\n",
            "merging (290, 326) into a new token 397\n",
            "merging ('.', '[') into a new token 398\n",
            "merging (398, '1') into a new token 399\n",
            "merging (399, ']') into a new token 400\n",
            "merging (352, ' ') into a new token 401\n",
            "merging ('ಿ', 308) into a new token 402\n",
            "merging (' ', 311) into a new token 403\n",
            "merging (265, 'ನ') into a new token 404\n",
            "merging (404, 'ು') into a new token 405\n",
            "merging (405, 353) into a new token 406\n",
            "merging (406, 'ಮ') into a new token 407\n",
            "merging (311, 314) into a new token 408\n",
            "merging ('ಲ', 258) into a new token 409\n",
            "merging (' ', 303) into a new token 410\n",
            "merging ('ಾ', 'ರ') into a new token 411\n",
            "merging ('ಗ', 285) into a new token 412\n",
            "merging (412, 273) into a new token 413\n",
            "merging (312, 'ತ') into a new token 414\n",
            "merging ('ಕ', 'ೆ') into a new token 415\n",
            "merging ('ಟ', 'ೋ') into a new token 416\n",
            "merging (416, 'ಕ') into a new token 417\n",
            "merging (323, 'ಾ') into a new token 418\n",
            "merging (418, 'ಜ') into a new token 419\n",
            "merging (419, 'ರ') into a new token 420\n",
            "merging (420, 267) into a new token 421\n",
            "merging (295, 302) into a new token 422\n",
            "merging (302, 'ಿ') into a new token 423\n",
            "merging (' ', 'ವ') into a new token 424\n",
            "merging ('ೆ', 312) into a new token 425\n",
            "merging (292, 359) into a new token 426\n",
            "merging (426, 360) into a new token 427\n",
            "merging (427, 361) into a new token 428\n",
            "merging (428, 314) into a new token 429\n",
            "merging (429, 366) into a new token 430\n",
            "merging (430, 'ರ') into a new token 431\n",
            "merging (431, 336) into a new token 432\n",
            "merging (432, 316) into a new token 433\n",
            "merging (433, 337) into a new token 434\n",
            "merging (434, 367) into a new token 435\n",
            "merging (435, 258) into a new token 436\n",
            "merging (436, 292) into a new token 437\n",
            "merging (437, 335) into a new token 438\n",
            "merging (438, 293) into a new token 439\n",
            "merging (439, 'ು') into a new token 440\n",
            "merging (440, 273) into a new token 441\n",
            "merging (441, 316) into a new token 442\n",
            "merging (442, 'ರ') into a new token 443\n",
            "merging (443, 265) into a new token 444\n",
            "merging (444, 257) into a new token 445\n",
            "merging (445, '-') into a new token 446\n",
            "merging (446, 332) into a new token 447\n",
            "merging (447, 361) into a new token 448\n",
            "merging (448, 368) into a new token 449\n",
            "merging (449, 303) into a new token 450\n",
            "merging (450, 304) into a new token 451\n",
            "merging (451, ' ') into a new token 452\n",
            "merging (452, 276) into a new token 453\n",
            "merging (453, 262) into a new token 454\n",
            "merging (454, 'ೂ') into a new token 455\n",
            "merging (455, 'ರ') into a new token 456\n",
            "merging (456, 258) into a new token 457\n",
            "merging (457, 303) into a new token 458\n",
            "merging (458, 'ಮ') into a new token 459\n",
            "merging (459, 369) into a new token 460\n",
            "merging (460, ' ') into a new token 461\n",
            "merging (461, 'ಘ') into a new token 462\n",
            "merging (462, 'ಟ') into a new token 463\n",
            "merging (463, 'ಕ') into a new token 464\n",
            "merging (464, 279) into a new token 465\n",
            "merging (465, 268) into a new token 466\n",
            "merging (466, 366) into a new token 467\n",
            "merging (467, 271) into a new token 468\n",
            "merging (468, ':') into a new token 469\n",
            "merging (469, ' ') into a new token 470\n",
            "merging (470, 359) into a new token 471\n",
            "merging (471, 370) into a new token 472\n",
            "merging (472, 374) into a new token 473\n",
            "merging (473, 262) into a new token 474\n",
            "merging (474, 369) into a new token 475\n",
            "merging (475, 375) into a new token 476\n",
            "merging (476, 'ಡ') into a new token 477\n",
            "merging (477, 318) into a new token 478\n",
            "merging (478, 376) into a new token 479\n",
            "merging (479, ' ') into a new token 480\n",
            "merging (480, 'ಯ') into a new token 481\n",
            "merging (481, 'ಾ') into a new token 482\n",
            "merging (482, 'ಂ') into a new token 483\n",
            "merging (483, 261) into a new token 484\n",
            "merging (484, 274) into a new token 485\n",
            "merging (485, 'ಕ') into a new token 486\n",
            "merging (486, 377) into a new token 487\n",
            "merging (487, 381) into a new token 488\n",
            "merging (488, 382) into a new token 489\n",
            "merging (489, 258) into a new token 490\n",
            "merging (490, 'ಫ') into a new token 491\n",
            "merging (491, 'ೀ') into a new token 492\n",
            "merging (492, 305) into a new token 493\n",
            "merging (493, '-') into a new token 494\n",
            "merging (494, 383) into a new token 495\n",
            "merging (495, 384) into a new token 496\n",
            "merging (496, 305) into a new token 497\n",
            "merging (497, 385) into a new token 498\n",
            "merging (498, 'ರ') into a new token 499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"tokens length:\", len(tokens))\n",
        "print(\"ids length:\", len(ids))\n",
        "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LCASIKgtg-3",
        "outputId": "2ebedb09-506e-4ddb-e38f-f2e876106032"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens length: 1252\n",
            "ids length: 363\n",
            "compression ratio: 3.45X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custome Byte pair encoding"
      ],
      "metadata": {
        "id": "ojqi5oG9Nspa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab(text):\n",
        "    \"\"\"Generate vocabulary from input text.\"\"\"\n",
        "    vocab = Counter()\n",
        "    for line in text:\n",
        "        words = line.strip().split()\n",
        "        for word in words:\n",
        "            # Add space between characters and append a special end-of-word token\n",
        "            word = \" \".join(list(word)) + \" <endoftext>\"\n",
        "            vocab[word] += 1\n",
        "    return vocab\n",
        "\n",
        "txt = \"hello I am testing this function\"\n",
        "get_vocab()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJl2n71eQ_ra",
        "outputId": "cb4ceef9-0986-447f-e731-8d6f48d90e86"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'h <endoftext>': 2,\n",
              "         'e <endoftext>': 2,\n",
              "         'l <endoftext>': 2,\n",
              "         'o <endoftext>': 2,\n",
              "         'I <endoftext>': 1,\n",
              "         'a <endoftext>': 1,\n",
              "         'm <endoftext>': 1,\n",
              "         't <endoftext>': 4,\n",
              "         's <endoftext>': 2,\n",
              "         'i <endoftext>': 3,\n",
              "         'n <endoftext>': 3,\n",
              "         'g <endoftext>': 1,\n",
              "         'f <endoftext>': 1,\n",
              "         'u <endoftext>': 1,\n",
              "         'c <endoftext>': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "\n",
        "def get_vocab(text):\n",
        "    \"\"\"Generate vocabulary from input text.\"\"\"\n",
        "    vocab = Counter()\n",
        "    for line in text:\n",
        "        words = line.strip().split()\n",
        "        for word in words:\n",
        "            # Add space between characters and append a special end-of-word token\n",
        "            word = \" \".join(list(word)) + \" <endoftext>\"\n",
        "            vocab[word] += 1\n",
        "    return vocab\n",
        "\n",
        "def get_stats(vocab):\n",
        "    \"\"\"Calculate pair frequencies in the vocabulary.\"\"\"\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, vocab):\n",
        "    \"\"\"Merge the most frequent pair in the vocabulary.\"\"\"\n",
        "    new_vocab = {}\n",
        "    bigram = re.escape(\" \".join(pair))  # Escape special characters\n",
        "    pattern = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")  # Match whole bigram\n",
        "\n",
        "    for word in vocab:\n",
        "        # Replace the bigram with the merged token\n",
        "        new_word = pattern.sub(\"\".join(pair), word)\n",
        "        new_vocab[new_word] = vocab[word]\n",
        "    return new_vocab\n",
        "\n",
        "def train_bpe(text, num_merges):\n",
        "    \"\"\"Train a Byte Pair Encoding model.\"\"\"\n",
        "    vocab = get_vocab(text)\n",
        "    for i in range(num_merges):\n",
        "        pairs = get_stats(vocab)\n",
        "        if not pairs:\n",
        "            break\n",
        "        best_pair = max(pairs, key=pairs.get)  # Find the most frequent pair\n",
        "        vocab = merge_vocab(best_pair, vocab)\n",
        "        print(f\"Step {i + 1}: Merged pair {best_pair}\")\n",
        "    return vocab\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    training_data = [\n",
        "        \"low low lower lowest\",\n",
        "        \"newer newer newer new\",\n",
        "        \"widest wider wide\"\n",
        "    ]\n",
        "    num_merges = 10\n",
        "    final_vocab = train_bpe(training_data, num_merges)\n",
        "\n",
        "    print(\"\\nFinal Vocabulary:\")\n",
        "    for word, freq in final_vocab.items():\n",
        "        print(f\"{word}: {freq}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD6dLscjtl1Q",
        "outputId": "04f44566-f19a-4c12-d0ea-b0421939ad20"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Merged pair ('w', 'e')\n",
            "Step 2: Merged pair ('r', '</w>')\n",
            "Step 3: Merged pair ('l', 'o')\n",
            "Step 4: Merged pair ('we', 'r</w>')\n",
            "Step 5: Merged pair ('n', 'e')\n",
            "Step 6: Merged pair ('w', '</w>')\n",
            "Step 7: Merged pair ('ne', 'wer</w>')\n",
            "Step 8: Merged pair ('w', 'i')\n",
            "Step 9: Merged pair ('wi', 'd')\n",
            "Step 10: Merged pair ('wid', 'e')\n",
            "\n",
            "Final Vocabulary:\n",
            "lo w</w>: 2\n",
            "lo wer</w>: 1\n",
            "lo we s t </w>: 1\n",
            "newer</w>: 3\n",
            "ne w</w>: 1\n",
            "wide s t </w>: 1\n",
            "wide r</w>: 1\n",
            "wide </w>: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYAWLWXiQpg1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}